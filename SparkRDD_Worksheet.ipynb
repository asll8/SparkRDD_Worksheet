{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3]) \n",
    "#parellelize(): a new distributed data set is created with specified number of partitions and \n",
    "#the elements of the collection are copied to the RDD.\n",
    "rdd.map(lambda x: x*x).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[[0, 0, 0], [1, 1, 1], [2, 4, 8], [3, 9, 27], [4, 16, 64]]\n"
     ]
    }
   ],
   "source": [
    "numbers = range(5)\n",
    "print(list(numbers))\n",
    "\n",
    "rdd = sc.parallelize(numbers)\n",
    "\n",
    "def square(a):\n",
    "    return [a,a*a,a*a*a]\n",
    "\n",
    "newRDD = rdd.map(lambda x: square(x))\n",
    "newArray = newRDD.collect()\n",
    "print(newArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5,5', '15,17', '15,16', '5,6', '6,6', '16,16', '7,7', '7,6', '17,17', '6,7', '7,8', '18,18', '8,6', '15,18', '5,8', '17,16', '8,8', '16,17', '17,18', '18,16']\n",
      "Before Sorted: \n",
      "[(5, 5), (15, 17), (15, 16), (5, 6), (6, 6), (16, 16), (7, 7), (7, 6), (17, 17), (6, 7), (7, 8), (18, 18), (8, 6), (15, 18), (5, 8), (17, 16), (8, 8), (16, 17), (17, 18), (18, 16)]\n",
      "After Sorted: \n",
      "[(5, 5), (5, 6), (6, 6), (7, 6), (8, 6), (7, 7), (6, 7), (7, 8), (5, 8), (8, 8), (15, 16), (16, 16), (17, 16), (18, 16), (15, 17), (17, 17), (16, 17), (18, 18), (15, 18), (17, 18)]\n"
     ]
    }
   ],
   "source": [
    "pointsRdd = sc.textFile(\"datasets/points.txt\")\n",
    "\n",
    "def splitAndMakeTuple(line):\n",
    "    arr = line.split(\",\")\n",
    "    x = int(arr[0])\n",
    "    y = int(arr[1])\n",
    "    return x,y\n",
    "\n",
    "print(pointsRdd.collect()) \n",
    "#collect(): gathered the splitted data\n",
    "newpointsRdd = pointsRdd.map(lambda x : splitAndMakeTuple(x))\n",
    "print(\"Before Sorted: \")\n",
    "print(newpointsRdd.collect())\n",
    "print(\"After Sorted: \")\n",
    "sortedRdd = newpointsRdd.sortBy(lambda arr : arr[1], ascending = True )\n",
    "print(sortedRdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Afghanistan,Asia,Kabul,29863010', 'Albania,Europe,Tirana,3129678', 'Algeria,Africa,Algiers,32853800']\n",
      "Before Sorted: \n",
      "[('Afghanistan', 29863010), ('Albania', 3129678), ('Algeria', 32853800), ('Andorra', 67151), ('Angola', 15941390), ('Antigua and Barbuda', 81479), ('Argentina', 38747150), ('Armenia', 3016312), ('Australia', 20155130), ('Austria', 8189444)]\n",
      "After Sorted: \n",
      "[(\"China People's Republic of\", 1315844000), ('India', 1103371000), ('United States', 298212900), ('Indonesia', 222781500), ('Brazil', 186404900), ('Pakistan', 157935100), ('Russia', 143201600), ('Bangladesh', 141822300), ('Nigeria', 131529700), ('Japan', 128084700)]\n"
     ]
    }
   ],
   "source": [
    "worldRDD = sc.textFile(\"datasets/world.txt\")\n",
    "\n",
    "def splitAndMakeTuple(line):\n",
    "    arr = line.split(\",\")\n",
    "    country = arr[0]\n",
    "    population = int(arr[3])\n",
    "    return(country,population)\n",
    "\n",
    "print(worldRDD.take(3))\n",
    "worldRdd = worldRDD.map(lambda line: splitAndMakeTuple(line))\n",
    "print(\"Before Sorted: \")\n",
    "print(worldRdd.take(10))\n",
    "print(\"After Sorted: \")\n",
    "sortedWorld = worldRdd.sortBy(lambda arr: arr[1],ascending = False)\n",
    "print(sortedWorld.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#Reduce\n",
    "\n",
    "rdd1 = sc.parallelize([4,3,6,7])\n",
    "result = rdd1.reduce(lambda x,y: x+y )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 12), ('c', 13), ('a', 8)]\n"
     ]
    }
   ],
   "source": [
    "#ReduceByKey\n",
    "\n",
    "rdd2 = sc.parallelize([('a',1),('b',2),('a',3),('a',4),('b',10),('c',5),('c',8)])\n",
    "result2 = rdd2.reduceByKey(lambda x,y: x+y)\n",
    "print(result2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 8), (15, 17), (15, 16), (15, 18), (16, 16), (16, 17), (17, 17), (17, 16), (17, 18), (18, 18), (18, 16)]\n"
     ]
    }
   ],
   "source": [
    "#Transformations\n",
    "\n",
    "pointsRdd = sc.textFile(\"datasets/points.txt\")\n",
    "\n",
    "def splitAndMakeTuple(line):\n",
    "    arr = line.split(\",\")\n",
    "    number1 = int(arr[0])\n",
    "    number2 = int(arr[1])\n",
    "    return (number1,number2)\n",
    "\n",
    "pointsRdd = pointsRdd.map(lambda x: splitAndMakeTuple(x))\n",
    "#print(pointsRdd.take(20))\n",
    "\n",
    "pointsRdd = pointsRdd.filter(lambda a: (a[0] + a[1]) > 15 )\n",
    "pointsRdd = pointsRdd.sortBy(lambda c: c[0], ascending = True)\n",
    "\n",
    "print(pointsRdd.take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('KURUTILEK- (ERZINCAN) [North East  3.0 km]', '1939.12.26', 7.9), ('ONIKI ADALAR (AKDENIZ)', '1926.06.26', 7.7), ('T�RKIYE-IRAN SINIR B�LGESI', '1930.05.06', 7.6), ('YENIYAKA-CALDIRAN (VAN) [South East  1.9 km]', '1976.11.24', 7.5), ('BASISKELE (KOCAELI) [North East  2.0 km]', '1999.08.17', 7.4), ('ERIKLICE-SARKOY (TEKIRDAG) [South East  4.3 km]', '1912.08.09', 7.3), ('YEMLICE- (VAN) [North West  1.5 km]', '2011.10.23', 7.2), ('UGUR- (DUZCE) [North East  0.3 km]', '1999.11.12', 7.2), ('SOGUCAK-YENICE (�ANAKKALE) [South West  2.3 km]', '1953.03.18', 7.2), ('AKDENIZ', '1948.02.09', 7.2)]\n"
     ]
    }
   ],
   "source": [
    "eaRdd = sc.textFile(\"datasets/DepremVerileri-2019-Nisan.txt\")\n",
    "eaRDD = eaRdd.filter(lambda line: \"Deprem Kodu\" not in line)\n",
    "#firstly, perform filtering to get better results\n",
    "def splitAndSelect(line):\n",
    "    arr = line.split(\"\\t\")\n",
    "    date = arr[2]\n",
    "    xM = float(arr[7])\n",
    "    place = arr[14]\n",
    "    return (place,date,xM)\n",
    "eaRdd = eaRDD.map(lambda x: splitAndSelect(x))\n",
    "eaRdd = eaRdd.sortBy(lambda arr: arr[2], ascending = False)\n",
    "\n",
    "print(eaRdd.take(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Area5', 7504), ('Area1', 2628), ('Area8', 1450), ('Area6', 1233), ('Area7', 1172), ('Area2', 959), ('Area3', 932), ('Area4', 718)]\n"
     ]
    }
   ],
   "source": [
    "earthRdd = sc.textFile(\"datasets/DepremVerileri-2019-Nisan.txt\")\n",
    "earthRdd = earthRdd.filter(lambda line: \"Deprem Kodu\" not in line)\n",
    "\n",
    "def splitAndSelect(line):\n",
    "    arr = line.split(\"\\t\")\n",
    "    long = float(arr[4])\n",
    "    lat = float(arr[5])\n",
    "    return (long,lat)\n",
    "\n",
    "earthRdd = earthRdd.map(lambda x: splitAndSelect(x))\n",
    "\n",
    "def findArea(x):\n",
    "    long = x[0]\n",
    "    lat = x[1]\n",
    "    area = \"Area1\"\n",
    "    \n",
    "    if (long > 39):\n",
    "        if (lat <= 31):\n",
    "            area = \"Area1\"\n",
    "        elif(lat <= 36):\n",
    "            area = \"Area2\"\n",
    "        elif (lat <= 41):\n",
    "            area = \"Area3\"\n",
    "        else:\n",
    "            area = \"Area4\"\n",
    "    else:\n",
    "        if (lat <= 31):\n",
    "            area = \"Area5\"\n",
    "        elif(lat <= 36):\n",
    "            area = \"Area6\"\n",
    "        elif (lat <= 41):\n",
    "            area = \"Area7\"\n",
    "        else:\n",
    "            area = \"Area8\"\n",
    "    return (area,1)\n",
    "\n",
    "earthRdd = earthRdd.map(lambda x: findArea(x))\n",
    "earthRdd = earthRdd.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "earthRdd = earthRdd.sortBy(lambda a: a[1], ascending = False)\n",
    "#earthRdd.saveAsTextFile(\"acb\")\n",
    "\n",
    "print(earthRdd.take(10))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sabancı', 'Üniversitesi,', 'Bilişim', 'Teknolojileri', '(BT)', 'Yüksek', 'Lisans', 'Programı', '', '(Professional', \"Masters'\", 'Degree', 'in', 'IT)', 'sektöre', 'nitelikli', 'iş', 'gücü', 'kazandırma', '', 'misyonu', 'ile', '2001', 'yılında', 'çalışmalarına', 'başlamış', 've', '2002', 'yılı', 'Güz', 'döneminde', '', 'eğitime', 'başlamıştır.', 'Program,', 'hızla', 'değişen', 'endüstri', 'gereksinimlerine', 'cevap', '', 'vermek', 'üzere', 'katılımcılarına', 'teknik', 'altyapı,', 'çözüm', 'üretme', 'yeteneği', 've', '', 'rekabet', 'gücü,', 'endüstriye', 'ise', 'bu', 'becerilere', 'sahip', 'profesyoneller', 'kazandırmayı', '', 'hedeflemektedir.', 'Eğitim', 'programının', 'tasarımında', 'endüstri', 'beklentileri', 've', 'yeni', '', 'teknolojiler', 'dikkate', 'alınarak', 'teorik', 'bilgilerin', 'uygulamalarla', 'desteklendiği,', '', 'laboratuvar', 'çalışmalarıyla', 'zenginleştirilmiş', 'bir', 'içerik', 've', 'format', 'benimsenmiştir.']\n"
     ]
    }
   ],
   "source": [
    "#FlatMap\n",
    "\n",
    "rdd3 = sc.textFile(\"datasets/text.txt\")\n",
    "rdd3 = rdd3.flatMap(lambda x: x.split(\" \"))\n",
    "print(rdd3.take(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 2, 2]\n",
      "[('a', (1, 2)), ('b', (1, 2)), ('c', (1, 2))]\n",
      "[(('a', 1), ('a', 2)), (('a', 1), ('b', 2)), (('a', 1), ('c', 2)), (('a', 1), ('e', 2)), (('b', 1), ('a', 2)), (('b', 1), ('b', 2)), (('b', 1), ('c', 2)), (('b', 1), ('e', 2)), (('c', 1), ('a', 2)), (('c', 1), ('b', 2))]\n",
      "-------\n",
      "[('a', 1, 'a', 2), ('a', 1, 'b', 2), ('a', 1, 'c', 2), ('a', 1, 'e', 2), ('b', 1, 'a', 2), ('b', 1, 'b', 2), ('b', 1, 'c', 2), ('b', 1, 'e', 2), ('c', 1, 'a', 2), ('c', 1, 'b', 2)]\n"
     ]
    }
   ],
   "source": [
    "#Union\n",
    "\n",
    "rdd4 = sc.parallelize([1,1,1])\n",
    "rdd5 = sc.parallelize([2,2,2])\n",
    "print(rdd4.union(rdd5).take(5))\n",
    "\n",
    "#Join\n",
    "\n",
    "rdd6 = sc.parallelize([(\"a\",1),(\"b\",1),(\"c\",1),(\"d\",1)])\n",
    "rdd7 = sc.parallelize([(\"a\",2),(\"b\",2),(\"c\",2),(\"e\",2)])\n",
    "print(rdd6.join(rdd7).take(10))\n",
    "\n",
    "#Cartesian\n",
    "\n",
    "print(rdd6.cartesian(rdd7).take(10))\n",
    "print(\"-------\")\n",
    "print(rdd6.cartesian(rdd7).map(lambda x: (x[0][0],x[0][1],x[1][0],x[1][1])).take(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', 1)]\n",
      "[('b', [1, 3]), ('c', [1]), ('a', [1, 5]), ('d', [1])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Intersection\n",
    "\n",
    "rdd1 = sc.parallelize([(\"a\",1),(\"b\",1),(\"c\",1),(\"d\",1)])\n",
    "rdd2 = sc.parallelize([(\"a\",2),(\"b\",2),(\"c\",1),(\"e\",2)])\n",
    "\n",
    "print(rdd1.intersection(rdd2).take(10))\n",
    "\n",
    "#GroupByKey\n",
    "\n",
    "rdd3 = sc.parallelize([(\"a\",1),(\"b\",1),(\"c\",1),(\"d\",1),(\"a\",5),(\"b\",3)])\n",
    "print(rdd3.groupByKey().map(lambda x: (x[0],list(x[1]))).take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24652\n",
      "[('the', 14946), ('of', 8239), ('and', 7251), ('a', 6470), ('to', 4991), ('in', 4888), ('he', 3778), ('his', 3295), ('i', 2658), ('that', 2554), ('with', 2507), ('', 2443), ('it', 2256), ('was', 2116), ('on', 2098), ('for', 1931), ('you', 1880), ('her', 1735), ('him', 1466), ('is', 1436), ('all', 1316), ('at', 1291), ('by', 1282), ('as', 1179), ('said', 1176), ('from', 1084), ('she', 1052), ('or', 1022), ('they', 1004), ('me', 899), ('be', 886), ('out', 880), ('not', 880), ('my', 826), ('what', 826), ('had', 812), ('up', 802), ('like', 723), ('their', 714), ('mr', 712), ('have', 695), ('but', 691), ('one', 682), ('there', 675), ('an', 654), ('them', 649), ('no', 628), ('so', 597), ('bloom', 589), ('then', 560), ('are', 549), ('if', 548), ('when', 544), ('about', 535), ('which', 509), ('were', 505), ('your', 490), ('old', 484), ('this', 482), ('says', 469), ('who', 463), ('down', 442), ('do', 436), ('over', 436), ('too', 427), ('we', 424), ('after', 421), ('now', 408), ('see', 407), ('man', 398), ('did', 398), ('would', 384), ('stephen', 380), ('two', 380), ('time', 359), ('off', 355), ('will', 347), ('back', 346), ('o', 345), ('yes', 337), ('into', 330), ('know', 319), ('other', 319), ('eyes', 317), ('those', 316), ('good', 315), ('some', 314), ('could', 311), ('more', 305), ('its', 304), ('where', 295), ('our', 292), ('has', 290), ('little', 288), ('bloom:', 281), ('hand', 277), ('street', 277), ('first', 271), ('way', 268), ('how', 268)]\n"
     ]
    }
   ],
   "source": [
    "#Word Count\n",
    "\n",
    "rdd=sc.textFile(\"datasets/JamesJoyce-Ulyses.txt\")\n",
    "rdd = rdd.filter(lambda line: len(line)>10)\n",
    "print(rdd.count())\n",
    "\n",
    "rdd = rdd.map(lambda line: line.replace(',','')\n",
    "             .replace('.','').replace('[','')\n",
    "             .replace('!','').replace(']','')\n",
    "             .replace('#','').replace('{','')\n",
    "             .replace('-','').replace('}',''))\n",
    "rdd = rdd.map(lambda line: line.lower())\n",
    "rdd = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "rdd = rdd.map(lambda x: (x,1))\n",
    "rdd = rdd.reduceByKey(lambda x,y: x+y)\n",
    "rdd = rdd.sortBy(lambda x : x[1],ascending=False)\n",
    "print(rdd.take(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
